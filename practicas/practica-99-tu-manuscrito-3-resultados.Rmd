---
title: "Práctica 99. Resultados de tu manuscrito."
author: José Ramón Martínez Batlle
date: 26-11-2022
output: github_document
bibliography: ../ref/biblio.bib
csl: ../ref/apa.csl
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, cache=F)
```

# ¿Qué contiene la sección "Resultados"?

Te recomiendo releer las normas para autores/as del [Anuario de Investigaciones Científicas de la UASD](../docs/instrucciones-para-autores-anuario-investigaciones-cientificas-UASD.pdf) y adherirte a las especificaciones sobre la sección "Resultados" que allí se indican. Aunque en las referidas normas no se incluyen muchas pautas significativas, si escribieras para una revista específica, deberás considerar sus normas y recomendaciones de publicación. Además, te recomiendo que consultes la sección "Resultados" de algunos manuscritos publicados en el Anuario.

En los resultados expones el contenido analítico central. Es "el qué" del manuscrito, en complemento de "el cómo" (metodología) y "el por qué" (introducción) de tu investigación. En los resultados muestras lo que encontraste luego de que colectaste (aunque en este caso, no fuiste al terreno) y analizaste, con tus métodos, los datos fuente.

Algunas recomendaciones generales:

- "Resultados" se supone que es la sección más corta del manuscrito, siempre que se usen apropiadamente los recursos gráficos, las tablas y la información suplementaria.

- Comienza por realizar tus análisis. Necesitarás una matriz de comunidad y una ambiental. La de comunidad la habrás generado en la práctica 2; la ambiental explico cómo generarla en este mismo cuaderno (ver abajo). En general, la matriz ambiental la producirás mediante estadísticos zonales del territorio dominicano. Para aprender más sobre la fuente de estadística zonal de República Dominicana, que contiene un conjunto de más de 100 variables resumidas por celdas H3, visita [este repo](https://github.com/geofis/zonal-statistics). Debes visitar dicho repo para poder citarlo apropiadamente.

- Cuando tengas análisis realizados, antes de comenzar a escribir, te recomiendo que escribas un guión de tu sección "Resultados". 

- Guión en mano, redacta tu sección "Resultados", siguiendo también estos consejos:

- En esta sección, se espera que presentes lo que has obtenido de manera "objetiva", evitando explicaciones, comentarios, opiniones, perspectivas o limitaciones. En teoría, tu redacción es "fría", lo cual no necesariamente significa que tenga que ser aburrida.

- Esta es la sección por excelencia donde usarás *tablas y/o gráficos*. Lo más importante a tener en cuenta cuando los uses es que no debes duplicar el contenido que muestran dichos recursos en el texto. La tabla o gráfico son apoyos que te ayudarán a no entrar en densidades innecesarias dentro de los párrafos. Por lo tanto, si colocas una tabla o figura, no caigas en la tentación de describirla en párrafos de forma exhausitva. Estos recursos deben servir para apoyar el o los párrafos donde destacas los principales patrones encontrados.

- *Importante también*: si insertas una tabla o gráfico, debes referirla en el texto (e.g. "ver figura X"). De nada sirve incluir una figura o una tabla si no la refieres, porque con ello estarás sugiriendo que dicho recurso era completamente prescindible.

- El tiempo verbal preferido (por defecto) es el pasado, por ejemplo "..., donde se **encontró** una asociación significativa entre ... y ...". Sin embargo, hay excepciones, como por ejemplo, cuando te refieres a una tabla o una figura. Un caso típico es la expresión "tal como se muestra en la tabla 1", donde el verbo está conjugado en presente.

A continuación, te pongo enlaces a referencias que considero útiles, sobre cómo redactar los resultados (algunas son generales, sobre artículos en general):

-   [Breves pautas, en inglés](https://www.editage.com/insights/the-secret-to-writing-the-results-and-discussion-section-of-a-manuscript). Puedes usar el traductor [DeepL](https://www.deepl.com/translator), porque produce frases más naturales.

Cinco guías, en inglés, que considero están bien elaboradas, sobre cómo redactar un artículo científico (consulta la sección sobre cómo redactar los *resultados* en cada una):

-   [Demystifying the Journal Article](https://www.insidehighered.com/advice/2017/05/09/how-write-effective-journal-article-and-get-it-published-essay)

-   [How to write a scientific manuscript for publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3626472/)

-   [11 steps to structuring a science paper editors will take seriously](https://www.elsevier.com/connect/11-steps-to-structuring-a-science-paper-editors-will-take-seriously)

-   [Cómo escribir un artículo científico por primera vez](https://www.sciencedirect.com/science/article/abs/pii/S1134593417300040) (necesitarás usar  [SciHub](https://sci-hub.se/) para descargarlo)

- Una muy breve pero con consejos útiles: [Tips for writing the perfect IMRAD manuscript](https://www.editage.com/insights/tips-for-writing-the-perfect-imrad-manuscript)


# Scripts de ejemplo

Una nota, a título informativo. Cada una se las siguientes secciones (e.g. "Análisis de agrupamiento", "Ordenación restringida"), es reproducible de forma autónoma, es decir, sin dependencia de líneas de código anteriores. Esto siginifica que las dependencias de una sección se resuelven dentro de ella misma, sin dependencia de líneas anteriores.

Por esta razón, varias secciones comparten texto y código común, pero sólo al inicio, porque al avanzar cada una se hace propia. Notarás, por lo tanto, que el texto "se repite a sí mismo" por esta razón; la otra opción que tenía era alojar el código común en un único archivo, pero entonces esto te obligaba a navegar por archivos separados, haciendo menos didáctico este cuaderno.

## Análisis de agrupamiento

Me basaré en los scripts que comienzan por `aa_` de este [repo](https://github.com/biogeografia-master/scripts-de-analisis-BCI), los cuales explico en los vídeos de "Análisis de agrupamiento" (del 13 al 16) de la lista de reproducción ["Ecología Numérica con R" de mi canal](https://www.youtube.com/playlist?list=PLDcT2n8UzsCRDqjqSeqHI1wsiNOqpYmsJ).


> INICIA texto+código común entre secciones

Fijar un directorio de trabajo no es recomendable, mejor trabaja por proyecto. En cualquier caso, si no quieres o no puedes crear un proyecto, usa la sentencia que verás abajo, cambiando `TU_DIRECTORIO` por la ruta del directorio donde tengas almacenados tus datos y tus scripts.

```{r}
# setwd('practicas/')
```

Cargar paquetes.

```{r}
library(vegan)
library(sf)
library(tidyverse)
library(tmap)
gh_content <- 'https://raw.githubusercontent.com/'
gh_zonal_stats <- 'https://github.com/geofis/zonal-statistics/raw/main/out/'
repo_analisis <- 'biogeografia-master/scripts-de-analisis-BCI/master'
devtools::source_url(paste0(gh_content, repo_analisis, '/biodata/funciones.R'))
source(paste0(gh_content, 'biogeografia-202202/material-de-apoyo/master/practicas/', 'train.R'))
```

Carga tu matriz de comunidad, que habrás generado en la práctica 2.

```{r}
mc_orig <- readRDS("matriz_de_comunidad.RDS")
nrow(mc_orig)
ncol(mc_orig)
names(mc_orig)
unique(word(names(mc_orig), 1, 1))
table(word(names(mc_orig), 1, 1))

# ¿En cuántos hexágonos está cada especie?
# Puede usar un único valor mínimo (inclusive) o un rango con dos números (extremos inclusive)
en_cuantos_hex <- 15
# En este caso, 15 significa 15 o más (hasta el límite superior). IMPORTANTE: elige TU PROPIO umbral.
# en_cuantos_hex <- 10:20
{if(length(en_cuantos_hex)==1) selector <- en_cuantos_hex:max(colSums(mc_orig)) else
  if(length(en_cuantos_hex)==2)
    selector <- min(en_cuantos_hex):max(en_cuantos_hex) else
      stop('Debes indicar uno o dos valores numéricos')}
selector
mi_fam <- mc_orig[, colSums(mc_orig) %in% selector]
ncol(mi_fam)
nombres_largos <- colnames(mi_fam)
(colnames(mi_fam) <- make.cepnames(word(colnames(mi_fam), 1, 2)))
(df_equivalencias <- data.frame(
  nombre_original = nombres_largos,
  colnames(mi_fam)))
```

Transforma la matriz de comunidad. Este paso es importante, lo explico [aquí](https://www.youtube.com/watch?v=yQ10lp0-nHc&list=PLDcT2n8UzsCRDqjqSeqHI1wsiNOqpYmsJ&index=10)

```{r}
mi_fam_t <- decostand(mi_fam, 'hellinger') #Hellinger
# Otras transformaciones posibles con datos de presencia/ausencia
# mi_fam_t <- decostand(mi_fam, 'normalize') #Chord
# mi_fam_t <- decostand(log1p(mi_fam), 'normalize') #Chord
# mi_fam_t <- decostand(mi_fam, 'chi.square') #Chi-square
```

Genera la matriz ambiental a partir del archivo de estadística zonal por celdas H3 de República Dominicana, de acuerdo con la resolución que prefieras. Para el ejemplo, usé la resolución 5, pero puedes usar/probar con otra, para lo cual, sólo tendrías que cambiar el objeto `res <- X`, donde `X` puede ser un número cualquiera entre 4 y 7.

Para aprender más sobre la fuente de estadística zonal de República Dominicana, que contiene un conjunto de más de 100 variables resumidas por celdas H3, visita [este repo](https://github.com/geofis/zonal-statistics). Debes visitar dicho repo para poder citarlo apropiadamente.

```{r, message=F, warning=F}
#Matriz ambiental
res <- 5 #Resolución H3
tmpfile <- tempfile()
download.file(paste0(gh_zonal_stats, 'all_sources_all_variables_res_', res, '.gpkg'), tmpfile)
za <- st_read(tmpfile, optional = T)
# Las siguientes líneas están comentadas, porque producen muchos mapas. Descoméntalas y ejecútalas si quieres verlos
# za %>% st_as_sf('geom') %>%
#   pivot_longer(cols = -matches('base|hex_id|geom')) %>% 
#   tm_shape() + tm_fill(col = 'value') +
#   tm_facets(by = 'name', free.scales = T)
za_intermedia <- za %>%
  st_drop_geometry() %>% 
  select(-matches(c(' base'))) %>% 
  column_to_rownames('hex_id')
env <- za_intermedia[match(rownames(mi_fam), rownames(za_intermedia)), ]
all(rownames(mi_fam) == rownames(env)) #Si es TRUE, sigue adelante
```

Se puede probar con un subconjunto de variables, generando una matriz ambiental que seleccione variables según el grupo al que pertenecen, con ayuda del sufijo.

```{r}
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^ESA '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^G90-GEOM '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^CH-BIO '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^GHH '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^GSL '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^CGL '))
```

> FINALIZA texto+código común entre secciones

A continuación, el **análisis de agrupamiento** propiamente. La parte más importante es generar un árbol, a partir de una matriz de distancias, que haga sentido desde el punto de vista de la comunidad y la distribución de las especies tal como se encuentran en GBIF. Primero cargaré paquetes específicos de esta técnica y generaré la matriz de distancias.

```{r}
library(broom)
library(cluster)
library(gclus)
library(pvclust)
mi_fam_d <- vegdist(mi_fam_t, "euc")
```

A continuación, generaré árboles usando distintos métodos, explicados en el [repo](https://github.com/biogeografia-master/scripts-de-analisis-BCI), y en los vídeos (13 a 16) de la lista mencionada arriba  ["Ecología Numérica con R" de mi canal](https://www.youtube.com/playlist?list=PLDcT2n8UzsCRDqjqSeqHI1wsiNOqpYmsJ).

```{r}
lista_cl <- list(
        cl_single = hclust(mi_fam_d, method = 'single'),
        cl_complete = hclust(mi_fam_d, method = 'complete'),
        cl_upgma = hclust(mi_fam_d, method = 'average'),
        cl_ward = hclust(mi_fam_d, method = 'ward.D2')
)
par(mfrow = c(2,2))
invisible(map(names(lista_cl), function(x) plot(lista_cl[[x]], main = x, hang = -1, cex = 0.4)))
par(mfrow = c(1,1))
```

A continuación, calcularé la distancia y la correlación cofenéticas. Se supone que el método con la mayor correlación cofenética explica mejor el agrupamiento de la comunidad. Consulta el vídeo y material de referencia. Normalmente, el método UPGMA obtiene la mayor correlación cofenética, pero esto se debe a que su procedimiento maximiza precisamente dicha métrica. No es recomendable conservar un único método de agrupamiento, normalmente es bueno usar al menos dos. Ward es muchas veces recomendado por basarse en procedimientos de cálculo muy distintos a los de UPGMA. Consulta referencias.

```{r}
map_df(lista_cl, function(x) {
        coph_d <- cophenetic(x)
        corr <- cor(mi_fam_d, coph_d)
        return(corr)
})
```

Ahora, calcularé las anchuras de silueta, una métrica que ayuda a determinar en cuántos grupos se organiza la comunidad. En el ejemplo, dado que muchas especies de Polygonaceae están ausentes en muchos hexágonos, es esperable que el procedimiento sugiera un número de grupos alto denominados (ver objeto impreso `n_grupos_optimos`). Considera la siguiente regla general: muchos grupos, y 6 o más se considera mucho, es un resultado poco útil; 1 grupo es un resultado sin sentido.

```{r}
anch_sil_upgma <- calcular_anchuras_siluetas(
        mc_orig = mi_fam, 
        distancias = mi_fam_d, 
        cluster = lista_cl$cl_upgma)
anch_sil_upgma

anch_sil_ward <- calcular_anchuras_siluetas(
        mc_orig = mi_fam, 
        distancias = mi_fam_d, 
        cluster = lista_cl$cl_ward)
anch_sil_ward
```

Por los resultados obtenidos, luce interesante explorar dos estrategias distintas: 1) Reducir el umbral de registros de presencia de especies raras; 2) Probar métodos "aproximadamente insesgados", basados en remuestreos y permutaciones.

Probaré lo segundo, pero te animo a que pruebes también 1), para lo cual, debes elegir un umbral (ya sea un valor mínimo o un rango) diferente al que inicialmente hayas elegido.

```{r}
cl_pvclust_upgma <-
        pvclust(t(mi_fam_t),
                method.hclust = "average",
                method.dist = "euc",
                iseed = 91, # Resultado reproducible
                parallel = TRUE)
# Añadir los valores de p
plot(cl_pvclust_upgma, hang = -1)
# Añadir rectángulos a los grupos significativos
lines(cl_pvclust_upgma)
pvrect(cl_pvclust_upgma, alpha = 0.90, border = 4)
```

Para UPGMA nos sugiere que hay más de 6 grupos, y esto se debe al gran número de hexágonos sin registros de presencia. No obstante, tratándose de registros sesgados como son los de GBIF, es esperado obtener este tipo de resultados, donde la comunidad luce muy atomizada.

```{r}
#' 
#' #### Ward
#' 
cl_pvclust_ward <-
        pvclust(t(mi_fam_t),
                method.hclust = "ward.D2",
                method.dist = "euc",
                iseed = 191, # Resultado reproducible
                parallel = TRUE)
# Añadir los valores de p
plot(cl_pvclust_ward, hang = -1)
# Añadir rectángulos a los grupos significativos
lines(cl_pvclust_ward)
pvrect(cl_pvclust_ward, alpha = 0.91, border = 4)
```




## Técnicas de ordenación

Me basaré en los scripts que comienzan por `to_` de este [repo](https://github.com/biogeografia-master/scripts-de-analisis-BCI), los cuales explico en los vídeos de "Técnicas de ordenación" de la lista de reproducción ["Ecología Numérica con R" de mi canal](https://www.youtube.com/playlist?list=PLDcT2n8UzsCRDqjqSeqHI1wsiNOqpYmsJ).

### Ordenación restringida

> INICIA texto+código común entre secciones

Fijar un directorio de trabajo no es recomendable, mejor trabaja por proyecto. En cualquier caso, si no quieres o no puedes crear un proyecto, usa la sentencia que verás abajo, cambiando `TU_DIRECTORIO` por la ruta del directorio donde tengas almacenados tus datos y tus scripts.

```{r}
# setwd('practicas/')
```

Cargar paquetes.

```{r}
library(vegan)
library(sf)
library(tidyverse)
library(tmap)
gh_content <- 'https://raw.githubusercontent.com/'
gh_zonal_stats <- 'https://github.com/geofis/zonal-statistics/raw/main/out/'
repo_analisis <- 'biogeografia-master/scripts-de-analisis-BCI/master'
devtools::source_url(paste0(gh_content, repo_analisis, '/biodata/funciones.R'))
source(paste0(gh_content, 'biogeografia-202202/material-de-apoyo/master/practicas/', 'train.R'))
```

Carga tu matriz de comunidad, que habrás generado en la práctica 2.

```{r}
mc_orig <- readRDS("matriz_de_comunidad.RDS")
nrow(mc_orig)
ncol(mc_orig)
names(mc_orig)
unique(word(names(mc_orig), 1, 1))
table(word(names(mc_orig), 1, 1))
sort(colSums(mc_orig))

# ¿En cuántos hexágonos está cada especie?
# Puede usar un único valor mínimo (inclusive) o un rango con dos números (extremos inclusive)
en_cuantos_hex <- 15
# En este caso, 15 significa 15 o más (hasta el límite superior). IMPORTANTE: elige TU PROPIO umbral.
# en_cuantos_hex <- 10:20
{if(length(en_cuantos_hex)==1) selector <- en_cuantos_hex:max(colSums(mc_orig)) else
  if(length(en_cuantos_hex)==2)
    selector <- min(en_cuantos_hex):max(en_cuantos_hex) else
      stop('Debes indicar uno o dos valores numéricos')}
selector
mi_fam <- mc_orig[, colSums(mc_orig) %in% selector]
ncol(mi_fam)
nombres_largos <- colnames(mi_fam)
(colnames(mi_fam) <- make.cepnames(word(colnames(mi_fam), 1, 2)))
(df_equivalencias <- data.frame(
  nombre_original = nombres_largos,
  colnames(mi_fam)))
```

Transforma la matriz de comunidad. Este paso es importante, lo explico [aquí](https://www.youtube.com/watch?v=yQ10lp0-nHc&list=PLDcT2n8UzsCRDqjqSeqHI1wsiNOqpYmsJ&index=10)

```{r}
mi_fam_t <- decostand(mi_fam, 'hellinger') #Hellinger
# Otras transformaciones posibles con datos de presencia/ausencia
# mi_fam_t <- decostand(mi_fam, 'normalize') #Chord
# mi_fam_t <- decostand(log1p(mi_fam), 'normalize') #Chord
# mi_fam_t <- decostand(mi_fam, 'chi.square') #Chi-square
```

Genera la matriz ambiental a partir del archivo de estadística zonal por celdas H3 de República Dominicana, de acuerdo con la resolución que prefieras. Para el ejemplo, usé la resolución 5, pero puedes usar/probar con otra, para lo cual, sólo tendrías que cambiar el objeto `res <- X`, donde `X` puede ser un número cualquiera entre 4 y 7.

Para aprender más sobre la fuente de estadística zonal de República Dominicana, que contiene un conjunto de más de 100 variables resumidas por celdas H3, visita [este repo](https://github.com/geofis/zonal-statistics). Debes visitar dicho repo para poder citarlo apropiadamente.

```{r}
#Matriz ambiental
res <- 5 #Resolución H3
tmpfile <- tempfile()
download.file(paste0(gh_zonal_stats, 'all_sources_all_variables_res_', res, '.gpkg'), tmpfile)
za <- st_read(tmpfile, optional = T)
za %>% st_as_sf('geom') %>%
  pivot_longer(cols = -matches('base|hex_id|geom')) %>% 
  tm_shape() + tm_fill(col = 'value') +
  tm_facets(by = 'name', free.scales = T)
za_intermedia <- za %>%
  st_drop_geometry() %>% 
  select(-matches(c(' base'))) %>% 
  column_to_rownames('hex_id')
env <- za_intermedia[match(rownames(mi_fam), rownames(za_intermedia)), ]
all(rownames(mi_fam) == rownames(env)) #Si es TRUE, sigue adelante
```

Se puede probar con un subconjunto de variables, generando una matriz ambiental que seleccione variables según el grupo al que pertenecen, con ayuda del sufijo.

```{r}
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^ESA '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^G90-GEOM '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^CH-BIO '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^GHH '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^GSL '))
# env_selecionada <- env %>%
#   st_drop_geometry() %>%
#   dplyr::select(matches('^CGL '))
```

> FINALIZA texto+código común entre secciones

A continuación, el análisis de ordenación propiamente. La parte más importante es el entrenamiento: la función `train` del paquete `caret`, contenida en la función `my_train`, simplifica la selección de variables. Lo más importante: prueba con todas las variables primero, observa las variables que recomienda el modelo final (`print_my_train(mod)`) y ensaya varias combinaciones de subconjuntos de variables.

```{r}
mi_fam_t_sel <- mi_fam_t %>%
  # select(matches('uvif|dive', ignore.case = T)) %>% #Serviría para filtrar la matriz de comunidad con esto
  rename_all(~ paste('ESPECIE', .x))
env_spp <- env %>% bind_cols(mi_fam_t_sel)
spp <- paste0('`', grep('^ESPECIE', colnames(env_spp), value = T), '`', collapse = ' + ')
my_formula <- as.formula(paste(spp, '~ .'))
set.seed(1); mod <- my_train(
  formula = my_formula, 
  preproceso = 'scale',
  data = env_spp %>%
    # select(matches('^GSL |^ESA |^ESPECIE ')) %>% #Sólo GSL y ESA, pero se debe explorar con todas
    select(matches('^ESA |^CH-BIO |^ESPECIE ')) %>% #Sólo ESA y CH-BIO, pero se debe explorar con todas
    select_all())
print_my_train(mod)
(covar <- grep(
  pattern = '\\(Intercept\\)',
  x = names(coef(mod$finalModel,unlist(mod$bestTune))),
  invert = T, value = T))
mi_fam_t_rda <- rda(mi_fam_t_sel %>% rename_all(~ gsub('^ESPECIE ', '', .)) ~ .,
                    env %>% select_at(all_of(gsub('\\`', '', covar))), scale = T)
summary(mi_fam_t_rda)
RsquareAdj(mi_fam_t_rda)$adj.r.squared
vif.cca(mi_fam_t_rda)
escalado <- 2
plot(mi_fam_t_rda,
     scaling = escalado,
     display = c("sp", "lc", "cn"),
     main = paste("Triplot de RDA especies ~ var. GSL + ESA, escalamiento", escalado)
)
mi_fam_t_rda_sc1 <- scores(mi_fam_t_rda,
         choices = 1:2,
         scaling = escalado,
         display = "sp"
  )
text(mi_fam_t_rda, "species", col="red", cex=0.8, scaling="sp")
arrows(0, 0,
       mi_fam_t_rda_sc1[, 1] * 0.9,
       mi_fam_t_rda_sc1[, 2] * 0.9,
       length = 0,
       lty = 1,
       col = "red"
)
```


## Análisis de diversidad

\ldots

## Ecología espacial

\ldots

# Referencias

